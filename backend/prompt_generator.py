"""
Japanese Prompt Generation System for Mac Status PWA
Converts system information and conversation context into Japanese prompts for ELYZA model
"""
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from enum import Enum
import re


class PromptStyle(Enum):
    """Different prompt styles for various interaction types"""
    CASUAL = "casual"           # ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªä¼šè©±
    TECHNICAL = "technical"     # æŠ€è¡“çš„ãªè©³ç´°
    FRIENDLY = "friendly"       # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå¯¾è©±
    PROFESSIONAL = "professional"  # ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«


class SystemMetricType(Enum):
    """Types of system metrics for focused responses"""
    CPU = "cpu"
    MEMORY = "memory"
    DISK = "disk"
    PROCESSES = "processes"
    NETWORK = "network"
    GENERAL = "general"


@dataclass
class ConversationContext:
    """Context information for conversation continuity"""
    user_name: Optional[str] = None
    preferred_style: PromptStyle = PromptStyle.FRIENDLY
    recent_topics: List[str] = None
    user_expertise_level: str = "beginner"  # beginner, intermediate, advanced
    conversation_history: List[Dict[str, str]] = None
    
    def __post_init__(self):
        if self.recent_topics is None:
            self.recent_topics = []
        if self.conversation_history is None:
            self.conversation_history = []


@dataclass
class PromptTemplate:
    """Template for generating prompts"""
    system_role: str
    context_format: str
    user_query_format: str
    response_guidelines: List[str]
    style_modifiers: Dict[str, str]


class JapanesePromptGenerator:
    """
    Generates Japanese prompts for ELYZA model based on system data and conversation context
    """
    
    def __init__(self):
        """Initialize the prompt generator with templates and formatting rules"""
        self.templates = self._initialize_templates()
        self.system_formatters = self._initialize_system_formatters()
        self.conversation_patterns = self._initialize_conversation_patterns()
        
    def _initialize_templates(self) -> Dict[PromptStyle, PromptTemplate]:
        """Initialize prompt templates for different styles"""
        return {
            PromptStyle.CASUAL: PromptTemplate(
                system_role="ã‚ãªãŸã¯Macã®çŠ¶æ…‹ã‚’ç›£è¦–ã™ã‚‹è¦ªã—ã¿ã‚„ã™ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ°—è»½ã«ä¼šè©±ã—ãªãŒã‚‰ã€ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¾ã™ã€‚",
                context_format="ç¾åœ¨ã®Macã®çŠ¶æ…‹:\n{system_info}\n",
                user_query_format="ãƒ¦ãƒ¼ã‚¶ãƒ¼: {query}\n",
                response_guidelines=[
                    "è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§è©±ã™",
                    "å°‚é–€ç”¨èªã¯åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹", 
                    "å¿…è¦ã«å¿œã˜ã¦çµµæ–‡å­—ã‚’ä½¿ç”¨ã™ã‚‹",
                    "ç°¡æ½”ã§ç†è§£ã—ã‚„ã™ã„å›ç­”ã‚’ã™ã‚‹"
                ],
                style_modifiers={
                    "greeting": "ã“ã‚“ã«ã¡ã¯ï¼",
                    "concern": "ã¡ã‚‡ã£ã¨æ°—ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã­",
                    "good_news": "è‰¯ã„æ„Ÿã˜ã§ã™ã­ï¼",
                    "suggestion": "ã“ã‚“ãªã“ã¨ã‚’è©¦ã—ã¦ã¿ã¦ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹"
                }
            ),
            
            PromptStyle.TECHNICAL: PromptTemplate(
                system_role="ã‚ãªãŸã¯Macã‚·ã‚¹ãƒ†ãƒ ã®æŠ€è¡“çš„ãªè©³ç´°ã«ç²¾é€šã—ãŸå°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ­£ç¢ºã§è©³ç´°ãªæŠ€è¡“æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚",
                context_format="ã‚·ã‚¹ãƒ†ãƒ è©³ç´°æƒ…å ±:\n{system_info}\n",
                user_query_format="ã‚¯ã‚¨ãƒª: {query}\n",
                response_guidelines=[
                    "æŠ€è¡“çš„ã«æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã™ã‚‹",
                    "å…·ä½“çš„ãªæ•°å€¤ã‚„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹",
                    "å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ç”¨ã™ã‚‹",
                    "è©³ç´°ãªåˆ†æã¨æ¨å¥¨äº‹é …ã‚’æä¾›ã™ã‚‹"
                ],
                style_modifiers={
                    "analysis": "ã‚·ã‚¹ãƒ†ãƒ åˆ†æçµæœ:",
                    "metrics": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹:",
                    "recommendation": "æŠ€è¡“çš„æ¨å¥¨äº‹é …:",
                    "warning": "æ³¨æ„ãŒå¿…è¦ãªé …ç›®:"
                }
            ),
            
            PromptStyle.FRIENDLY: PromptTemplate(
                system_role="ã‚ãªãŸã¯Macãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é ¼ã‚Œã‚‹å‹äººã®ã‚ˆã†ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ¸©ã‹ãè¦ªåˆ‡ã«ã€ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚",
                context_format="ã‚ãªãŸã®Macã®ä»Šã®æ§˜å­:\n{system_info}\n",
                user_query_format="è³ªå•: {query}\n",
                response_guidelines=[
                    "æ¸©ã‹ãè¦ªåˆ‡ãªå£èª¿ã§å¯¾å¿œã™ã‚‹",
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒé…äº‹ã«å…±æ„Ÿã™ã‚‹",
                    "åˆ†ã‹ã‚Šã‚„ã™ã„ä¾‹ãˆã‚’ä½¿ç”¨ã™ã‚‹",
                    "å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹å›ç­”ã‚’ã™ã‚‹"
                ],
                style_modifiers={
                    "reassurance": "ã”å®‰å¿ƒãã ã•ã„",
                    "explanation": "ç°¡å˜ã«èª¬æ˜ã™ã‚‹ã¨",
                    "help": "ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™",
                    "status": "ç¾åœ¨ã®çŠ¶æ³ã¯"
                }
            ),
            
            PromptStyle.PROFESSIONAL: PromptTemplate(
                system_role="ã‚ãªãŸã¯Macã‚·ã‚¹ãƒ†ãƒ ç®¡ç†ã®å°‚é–€å®¶ã¨ã—ã¦ã€ãƒ“ã‚¸ãƒã‚¹ç’°å¢ƒã§ã®ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚",
                context_format="ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ:\n{system_info}\n",
                user_query_format="ãŠå•ã„åˆã‚ã›: {query}\n",
                response_guidelines=[
                    "ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ä¸å¯§ãªè¨€è‘‰é£ã„",
                    "ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿ã‚’è€ƒæ…®ã—ãŸå›ç­”",
                    "å…·ä½“çš„ãªå¯¾å‡¦æ³•ã‚’æç¤º",
                    "ãƒªã‚¹ã‚¯è©•ä¾¡ã‚’å«ã‚ã‚‹"
                ],
                style_modifiers={
                    "report": "ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³å ±å‘Š:",
                    "impact": "æ¥­å‹™ã¸ã®å½±éŸ¿:",
                    "action": "æ¨å¥¨å¯¾å¿œ:",
                    "priority": "å„ªå…ˆåº¦:"
                }
            )
        }
    
    def _initialize_system_formatters(self) -> Dict[str, callable]:
        """Initialize system information formatters"""
        return {
            'cpu': self._format_cpu_info,
            'memory': self._format_memory_info,
            'disk': self._format_disk_info,
            'processes': self._format_process_info,
            'network': self._format_network_info,
            'general': self._format_general_info
        }
    
    def _initialize_conversation_patterns(self) -> Dict[str, List[str]]:
        """Initialize conversation patterns for context awareness"""
        return {
            'greetings': [
                'ã“ã‚“ã«ã¡ã¯', 'ãŠã¯ã‚ˆã†', 'ã“ã‚“ã°ã‚“ã¯', 'ã¯ã˜ã‚ã¾ã—ã¦',
                'hello', 'hi', 'hey'
            ],
            'status_requests': [
                'çŠ¶æ…‹', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'æ§˜å­', 'èª¿å­', 'å…·åˆ',
                'status', 'condition', 'how'
            ],
            'performance_concerns': [
                'é…ã„', 'é‡ã„', 'å‹•ã‹ãªã„', 'å•é¡Œ', 'ã‚¨ãƒ©ãƒ¼',
                'slow', 'heavy', 'problem', 'error', 'issue'
            ],
            'resource_queries': [
                'CPU', 'ãƒ¡ãƒ¢ãƒª', 'ãƒ‡ã‚£ã‚¹ã‚¯', 'ãƒ—ãƒ­ã‚»ã‚¹', 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯',
                'memory', 'disk', 'process', 'network'
            ]
        }
    
    def generate_system_prompt(self, 
                             user_query: str,
                             system_data: Dict[str, Any],
                             context: Optional[ConversationContext] = None,
                             focus_metric: Optional[SystemMetricType] = None) -> str:
        """
        Generate a complete Japanese prompt for the ELYZA model
        
        Args:
            user_query: User's input query
            system_data: Current system status data
            context: Conversation context for personalization
            focus_metric: Specific metric to focus on
            
        Returns:
            Complete formatted prompt string
        """
        if context is None:
            context = ConversationContext()
        
        # Determine appropriate style based on query and context
        style = self._determine_prompt_style(user_query, context)
        template = self.templates[style]
        
        # Format system information
        system_info = self._format_system_information(system_data, style, focus_metric)
        
        # Build conversation context
        conversation_context = self._build_conversation_context(context)
        
        # Generate the complete prompt
        prompt_parts = [
            template.system_role,
            "",
            template.context_format.format(system_info=system_info),
        ]
        
        # Add conversation history if available
        if conversation_context:
            prompt_parts.extend([
                "ä¼šè©±å±¥æ­´:",
                conversation_context,
                ""
            ])
        
        # Add response guidelines
        guidelines = "å›ç­”æ™‚ã®æ³¨æ„ç‚¹:\n" + "\n".join(f"- {guideline}" for guideline in template.response_guidelines)
        prompt_parts.extend([guidelines, ""])
        
        # Add user query
        prompt_parts.append(template.user_query_format.format(query=user_query))
        
        # Add response starter
        prompt_parts.append("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: ")
        
        return "\n".join(prompt_parts)
    
    def _determine_prompt_style(self, user_query: str, context: ConversationContext) -> PromptStyle:
        """Determine the most appropriate prompt style based on query and context"""
        query_lower = user_query.lower()
        
        # Check for technical keywords
        technical_keywords = ['è©³ç´°', 'ã‚¹ãƒšãƒƒã‚¯', 'æŠ€è¡“', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'ãƒ¡ãƒˆãƒªã‚¯ã‚¹', 'ãƒ­ã‚°']
        if any(keyword in query_lower for keyword in technical_keywords):
            return PromptStyle.TECHNICAL
        
        # Check for professional context
        professional_keywords = ['ãƒ¬ãƒãƒ¼ãƒˆ', 'å ±å‘Š', 'ãƒ“ã‚¸ãƒã‚¹', 'æ¥­å‹™', 'ä¼šç¤¾']
        if any(keyword in query_lower for keyword in professional_keywords):
            return PromptStyle.PROFESSIONAL
        
        # Check for casual indicators
        casual_keywords = ['ã©ã†', 'ãªã‚“ã‹', 'ã¡ã‚‡ã£ã¨', 'ğŸ˜Š', 'ğŸ‘']
        if any(keyword in query_lower for keyword in casual_keywords):
            return PromptStyle.CASUAL
        
        # Default to user's preferred style or friendly
        return context.preferred_style
    
    def _format_system_information(self, 
                                 system_data: Dict[str, Any], 
                                 style: PromptStyle,
                                 focus_metric: Optional[SystemMetricType] = None) -> str:
        """Format system information according to style and focus"""
        formatted_parts = []
        
        if focus_metric:
            # Focus on specific metric
            if focus_metric.value in self.system_formatters:
                formatter = self.system_formatters[focus_metric.value]
                formatted_parts.append(formatter(system_data, style))
        else:
            # Include all relevant system information
            for metric_type, formatter in self.system_formatters.items():
                if metric_type != 'general':  # General is used for overview
                    formatted_info = formatter(system_data, style)
                    if formatted_info:
                        formatted_parts.append(formatted_info)
        
        return "\n".join(formatted_parts)
    
    def _format_cpu_info(self, system_data: Dict[str, Any], style: PromptStyle) -> str:
        """Format CPU information"""
        if 'cpu_percent' not in system_data:
            return ""
        
        cpu_percent = system_data['cpu_percent']
        cpu_count = system_data.get('cpu_count', 'N/A')
        
        # Handle invalid data types
        try:
            cpu_percent = float(cpu_percent)
        except (ValueError, TypeError):
            return "CPUä½¿ç”¨ç‡: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼"
        
        if style == PromptStyle.TECHNICAL:
            return f"CPUä½¿ç”¨ç‡: {cpu_percent:.1f}% (ã‚³ã‚¢æ•°: {cpu_count})"
        elif style == PromptStyle.CASUAL:
            if cpu_percent > 80:
                return f"CPU: {cpu_percent:.1f}% - ã¡ã‚‡ã£ã¨å¿™ã—ãã†ã§ã™ã­ ğŸ˜…"
            elif cpu_percent > 50:
                return f"CPU: {cpu_percent:.1f}% - ã¾ã‚ã¾ã‚åƒã„ã¦ã¾ã™"
            else:
                return f"CPU: {cpu_percent:.1f}% - ä½™è£•ãŒã‚ã‚Šã¾ã™ã­ ğŸ‘"
        else:
            return f"CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%"
    
    def _format_memory_info(self, system_data: Dict[str, Any], style: PromptStyle) -> str:
        """Format memory information"""
        if 'memory_percent' not in system_data:
            return ""
        
        memory_percent = system_data['memory_percent']
        memory_used = system_data.get('memory_used', 0)
        memory_total = system_data.get('memory_total', 0)
        
        # Handle invalid data types
        try:
            memory_percent = float(memory_percent) if memory_percent is not None else 0.0
            memory_used = float(memory_used) if memory_used is not None else 0.0
            memory_total = float(memory_total) if memory_total is not None else 0.0
        except (ValueError, TypeError):
            return "ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼"
        
        if memory_total > 0:
            used_gb = memory_used / (1024**3)
            total_gb = memory_total / (1024**3)
            
            if style == PromptStyle.TECHNICAL:
                return f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {used_gb:.1f}GB / {total_gb:.1f}GB ({memory_percent:.1f}%)"
            elif style == PromptStyle.CASUAL:
                if memory_percent > 85:
                    return f"ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}% - ãã‚ãã‚ã„ã£ã±ã„ã‹ã‚‚ ğŸ’­"
                elif memory_percent > 70:
                    return f"ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}% - çµæ§‹ä½¿ã£ã¦ã¾ã™ã­"
                else:
                    return f"ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}% - ã¾ã ä½™è£•ãŒã‚ã‚Šã¾ã™"
            else:
                return f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory_percent:.1f}% ({used_gb:.1f}GB / {total_gb:.1f}GB)"
        else:
            return f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory_percent:.1f}%"
    
    def _format_disk_info(self, system_data: Dict[str, Any], style: PromptStyle) -> str:
        """Format disk information"""
        if 'disk_percent' not in system_data:
            return ""
        
        disk_percent = system_data['disk_percent']
        disk_used = system_data.get('disk_used', 0)
        disk_total = system_data.get('disk_total', 0)
        
        if disk_total > 0:
            used_gb = disk_used / (1024**3)
            total_gb = disk_total / (1024**3)
            
            if style == PromptStyle.TECHNICAL:
                return f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡: {used_gb:.1f}GB / {total_gb:.1f}GB ({disk_percent:.1f}%)"
            elif style == PromptStyle.CASUAL:
                if disk_percent > 90:
                    return f"ãƒ‡ã‚£ã‚¹ã‚¯: {disk_percent:.1f}% - ãã‚ãã‚ãŠæƒé™¤ãŒå¿…è¦ã‹ã‚‚ ğŸ§¹"
                elif disk_percent > 75:
                    return f"ãƒ‡ã‚£ã‚¹ã‚¯: {disk_percent:.1f}% - ã ã„ã¶ä½¿ã£ã¦ã¾ã™ã­"
                else:
                    return f"ãƒ‡ã‚£ã‚¹ã‚¯: {disk_percent:.1f}% - ã¾ã å¤§ä¸ˆå¤«ã§ã™"
            else:
                return f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡: {disk_percent:.1f}%"
        else:
            return f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡: {disk_percent:.1f}%"
    
    def _format_process_info(self, system_data: Dict[str, Any], style: PromptStyle) -> str:
        """Format process information"""
        if 'top_processes' not in system_data or not system_data['top_processes']:
            return ""
        
        # Handle invalid data types
        try:
            if not isinstance(system_data['top_processes'], list):
                return ""
            top_processes = system_data['top_processes'][:3]  # Top 3 processes
        except (TypeError, AttributeError):
            return ""
        
        if style == PromptStyle.TECHNICAL:
            process_lines = []
            for i, proc in enumerate(top_processes, 1):
                name = proc.get('name', 'Unknown')
                cpu = proc.get('cpu_percent', 0)
                memory = proc.get('memory_percent', 0)
                process_lines.append(f"{i}. {name} (CPU: {cpu:.1f}%, Memory: {memory:.1f}%)")
            return "ä¸Šä½ãƒ—ãƒ­ã‚»ã‚¹:\n" + "\n".join(process_lines)
        
        elif style == PromptStyle.CASUAL:
            top_proc = top_processes[0]
            name = top_proc.get('name', 'Unknown')
            cpu = top_proc.get('cpu_percent', 0)
            
            if cpu > 50:
                return f"ä¸€ç•ªå¿™ã—ã„ã‚¢ãƒ—ãƒª: {name} ({cpu:.1f}%) - é ‘å¼µã£ã¦ã¾ã™ã­ï¼"
            else:
                return f"ä¸€ç•ªæ´»ç™ºãªã‚¢ãƒ—ãƒª: {name} ({cpu:.1f}%)"
        
        else:
            top_proc = top_processes[0]
            name = top_proc.get('name', 'Unknown')
            cpu = top_proc.get('cpu_percent', 0)
            return f"æœ€ã‚‚CPUã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ—ãƒ­ã‚»ã‚¹: {name} ({cpu:.1f}%)"
    
    def _format_network_info(self, system_data: Dict[str, Any], style: PromptStyle) -> str:
        """Format network information"""
        if 'network_io' not in system_data:
            return ""
        
        network = system_data['network_io']
        if isinstance(network, dict):
            sent_mb = network.get('bytes_sent', 0) / (1024**2)
            recv_mb = network.get('bytes_recv', 0) / (1024**2)
            
            if style == PromptStyle.TECHNICAL:
                return f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ I/O: é€ä¿¡ {sent_mb:.1f}MB, å—ä¿¡ {recv_mb:.1f}MB"
            elif style == PromptStyle.CASUAL:
                total_mb = sent_mb + recv_mb
                if total_mb > 1000:
                    return f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: {total_mb:.0f}MB - ã‚ˆãé€šä¿¡ã—ã¦ã¾ã™ã­ ğŸ“¡"
                else:
                    return f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: {total_mb:.0f}MB"
            else:
                return f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡é‡: é€ä¿¡ {sent_mb:.1f}MB, å—ä¿¡ {recv_mb:.1f}MB"
        
        return ""
    
    def _format_general_info(self, system_data: Dict[str, Any], style: PromptStyle) -> str:
        """Format general system overview"""
        timestamp = system_data.get('timestamp')
        if timestamp:
            if isinstance(timestamp, str):
                time_str = timestamp
            else:
                time_str = timestamp.strftime('%H:%M:%S')
            
            if style == PromptStyle.CASUAL:
                return f"ğŸ“Š {time_str} ç¾åœ¨ã®çŠ¶æ³"
            else:
                return f"å–å¾—æ™‚åˆ»: {time_str}"
        
        return ""
    
    def _build_conversation_context(self, context: ConversationContext) -> str:
        """Build conversation context string"""
        if not context.conversation_history:
            return ""
        
        # Get recent conversation (last 3 exchanges)
        recent_history = context.conversation_history[-6:]  # 3 exchanges = 6 messages
        
        context_lines = []
        for msg in recent_history:
            role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg.get('role') == 'user' else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
            content = msg.get('content', '')[:100]  # Limit length
            if len(msg.get('content', '')) > 100:
                content += "..."
            context_lines.append(f"{role}: {content}")
        
        return "\n".join(context_lines)
    
    def create_focused_prompt(self, 
                            user_query: str,
                            system_data: Dict[str, Any],
                            metric_type: SystemMetricType,
                            context: Optional[ConversationContext] = None) -> str:
        """
        Create a prompt focused on a specific system metric
        
        Args:
            user_query: User's query
            system_data: System information
            metric_type: Type of metric to focus on
            context: Conversation context
            
        Returns:
            Focused prompt string
        """
        return self.generate_system_prompt(
            user_query=user_query,
            system_data=system_data,
            context=context,
            focus_metric=metric_type
        )
    
    def create_comparison_prompt(self,
                               user_query: str,
                               current_data: Dict[str, Any],
                               previous_data: Dict[str, Any],
                               context: Optional[ConversationContext] = None) -> str:
        """
        Create a prompt that compares current and previous system states
        
        Args:
            user_query: User's query
            current_data: Current system data
            previous_data: Previous system data
            context: Conversation context
            
        Returns:
            Comparison prompt string
        """
        if context is None:
            context = ConversationContext()
        
        style = self._determine_prompt_style(user_query, context)
        template = self.templates[style]
        
        # Format current and previous data
        current_info = self._format_system_information(current_data, style)
        previous_info = self._format_system_information(previous_data, style)
        
        # Create comparison context
        comparison_context = f"ç¾åœ¨ã®çŠ¶æ…‹:\n{current_info}\n\nä»¥å‰ã®çŠ¶æ…‹:\n{previous_info}\n"
        
        # Build conversation context
        conversation_context = self._build_conversation_context(context)
        
        # Generate comparison prompt
        prompt_parts = [
            template.system_role + " ç¾åœ¨ã®çŠ¶æ…‹ã¨ä»¥å‰ã®çŠ¶æ…‹ã‚’æ¯”è¼ƒã—ã¦ã€å¤‰åŒ–ã‚„å‚¾å‘ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "",
            comparison_context,
        ]
        
        if conversation_context:
            prompt_parts.extend([
                "ä¼šè©±å±¥æ­´:",
                conversation_context,
                ""
            ])
        
        prompt_parts.extend([
            "æ¯”è¼ƒåˆ†æã®æ³¨æ„ç‚¹:",
            "- é‡è¦ãªå¤‰åŒ–ã‚’ç‰¹å®šã™ã‚‹",
            "- æ”¹å–„ç‚¹ã‚„æ‚ªåŒ–ç‚¹ã‚’æ˜ç¢ºã«ã™ã‚‹", 
            "- å¿…è¦ã«å¿œã˜ã¦å¯¾å‡¦æ³•ã‚’ææ¡ˆã™ã‚‹",
            "",
            template.user_query_format.format(query=user_query),
            "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "
        ])
        
        return "\n".join(prompt_parts)
    
    def extract_query_intent(self, user_query: str) -> Dict[str, Any]:
        """
        Extract intent and entities from user query
        
        Args:
            user_query: User's input query
            
        Returns:
            Dictionary with intent and extracted information
        """
        query_lower = user_query.lower()
        
        intent_info = {
            'primary_intent': 'general_inquiry',
            'metric_focus': None,
            'urgency_level': 'normal',
            'response_type': 'informative',
            'entities': []
        }
        
        # Detect metric focus
        for pattern_type, keywords in self.conversation_patterns.items():
            for keyword in keywords:
                if keyword.lower() in query_lower:
                    if pattern_type == 'resource_queries':
                        if 'cpu' in keyword.lower():
                            intent_info['metric_focus'] = SystemMetricType.CPU
                        elif 'ãƒ¡ãƒ¢ãƒª' in keyword or 'memory' in keyword.lower():
                            intent_info['metric_focus'] = SystemMetricType.MEMORY
                        elif 'ãƒ‡ã‚£ã‚¹ã‚¯' in keyword or 'disk' in keyword.lower():
                            intent_info['metric_focus'] = SystemMetricType.DISK
                        elif 'ãƒ—ãƒ­ã‚»ã‚¹' in keyword or 'process' in keyword.lower():
                            intent_info['metric_focus'] = SystemMetricType.PROCESSES
                        elif 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯' in keyword or 'network' in keyword.lower():
                            intent_info['metric_focus'] = SystemMetricType.NETWORK
                    
                    intent_info['entities'].append({
                        'type': pattern_type,
                        'value': keyword,
                        'category': pattern_type
                    })
        
        # Detect urgency
        urgent_keywords = ['ç·Šæ€¥', 'æ€¥ã„ã§', 'å•é¡Œ', 'ã‚¨ãƒ©ãƒ¼', 'å‹•ã‹ãªã„', 'é…ã„', 'é‡ã„', 'ï¼', 'ã‚¯ãƒ©ãƒƒã‚·ãƒ¥', 'åœæ­¢']
        if any(keyword in query_lower for keyword in urgent_keywords):
            intent_info['urgency_level'] = 'high'
        
        # Detect response type preference
        if any(word in query_lower for word in ['è©³ã—ã', 'è©³ç´°', 'å…·ä½“çš„']):
            intent_info['response_type'] = 'detailed'
        elif any(word in query_lower for word in ['ç°¡å˜', 'è¦ç´„', 'çŸ­ã']):
            intent_info['response_type'] = 'brief'
        
        return intent_info


# Utility functions for common prompt generation scenarios
def create_status_check_prompt(system_data: Dict[str, Any], 
                             style: PromptStyle = PromptStyle.FRIENDLY) -> str:
    """Create a prompt for general status check"""
    generator = JapanesePromptGenerator()
    context = ConversationContext(preferred_style=style)
    
    return generator.generate_system_prompt(
        user_query="ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ã‚’æ•™ãˆã¦ãã ã•ã„",
        system_data=system_data,
        context=context
    )


def create_performance_analysis_prompt(system_data: Dict[str, Any],
                                     performance_issues: List[str] = None) -> str:
    """Create a prompt for performance analysis"""
    generator = JapanesePromptGenerator()
    context = ConversationContext(preferred_style=PromptStyle.TECHNICAL)
    
    query = "ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æã—ã¦ãã ã•ã„"
    if performance_issues:
        query += f"ã€‚ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«ã¤ã„ã¦: {', '.join(performance_issues)}"
    
    return generator.generate_system_prompt(
        user_query=query,
        system_data=system_data,
        context=context
    )


def create_troubleshooting_prompt(system_data: Dict[str, Any],
                                issue_description: str) -> str:
    """Create a prompt for troubleshooting assistance"""
    generator = JapanesePromptGenerator()
    context = ConversationContext(preferred_style=PromptStyle.PROFESSIONAL)
    
    query = f"æ¬¡ã®å•é¡Œã«ã¤ã„ã¦å¯¾å‡¦æ³•ã‚’æ•™ãˆã¦ãã ã•ã„: {issue_description}"
    
    return generator.generate_system_prompt(
        user_query=query,
        system_data=system_data,
        context=context
    )


# Test function
async def test_prompt_generator():
    """Test function for JapanesePromptGenerator"""
    print("ğŸ§ª Testing Japanese Prompt Generator")
    print("=" * 50)
    
    # Sample system data
    sample_system_data = {
        'timestamp': datetime.now(),
        'cpu_percent': 45.2,
        'cpu_count': 8,
        'memory_percent': 68.5,
        'memory_used': 11 * 1024**3,  # 11GB
        'memory_total': 16 * 1024**3,  # 16GB
        'disk_percent': 72.1,
        'disk_used': 360 * 1024**3,   # 360GB
        'disk_total': 500 * 1024**3,  # 500GB
        'top_processes': [
            {'name': 'Google Chrome', 'cpu_percent': 25.3, 'memory_percent': 15.2},
            {'name': 'Xcode', 'cpu_percent': 12.1, 'memory_percent': 8.7},
            {'name': 'Finder', 'cpu_percent': 3.2, 'memory_percent': 2.1}
        ],
        'network_io': {
            'bytes_sent': 150 * 1024**2,  # 150MB
            'bytes_recv': 300 * 1024**2   # 300MB
        }
    }
    
    generator = JapanesePromptGenerator()
    
    # Test different styles
    test_cases = [
        ("ã‚·ã‚¹ãƒ†ãƒ ã®èª¿å­ã¯ã©ã†ã§ã™ã‹ï¼Ÿ", PromptStyle.CASUAL),
        ("è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚’ãŠé¡˜ã„ã—ã¾ã™", PromptStyle.TECHNICAL),
        ("ç¾åœ¨ã®çŠ¶æ³ã‚’æ•™ãˆã¦ãã ã•ã„", PromptStyle.FRIENDLY),
        ("ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„", PromptStyle.PROFESSIONAL)
    ]
    
    for i, (query, style) in enumerate(test_cases, 1):
        print(f"\n{i}. Test Case: {style.value.upper()}")
        print("-" * 30)
        
        context = ConversationContext(
            preferred_style=style,
            user_expertise_level="intermediate",
            conversation_history=[
                {"role": "user", "content": "ã“ã‚“ã«ã¡ã¯"},
                {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}
            ]
        )
        
        prompt = generator.generate_system_prompt(
            user_query=query,
            system_data=sample_system_data,
            context=context
        )
        
        print(f"Query: {query}")
        print(f"Generated Prompt Length: {len(prompt)} characters")
        print(f"Preview: {prompt[:200]}...")
        
        # Test intent extraction
        intent = generator.extract_query_intent(query)
        print(f"Detected Intent: {intent['primary_intent']}")
        if intent['metric_focus']:
            print(f"Metric Focus: {intent['metric_focus'].value}")
    
    # Test focused prompts
    print(f"\n5. Test Focused Prompt (CPU)")
    print("-" * 30)
    
    focused_prompt = generator.create_focused_prompt(
        user_query="CPUã®ä½¿ç”¨çŠ¶æ³ã«ã¤ã„ã¦æ•™ãˆã¦",
        system_data=sample_system_data,
        metric_type=SystemMetricType.CPU
    )
    
    print(f"Focused Prompt Length: {len(focused_prompt)} characters")
    print(f"Preview: {focused_prompt[:200]}...")
    
    print("\nâœ… All tests completed successfully!")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_prompt_generator())