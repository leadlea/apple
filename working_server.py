#!/usr/bin/env python3
"""
Working Mac Status PWA Server - Ollama + Smalltalk Chat + Warmup
- é›‘è«‡ã¯ /api/chat ã§è»½é‡å¿œç­”
- ã‚·ã‚¹ãƒ†ãƒ è³ªå•ã¯ /api/generateï¼ˆå¾“æ¥ã©ãŠã‚Šï¼‰ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãå¿œç­”
- èµ·å‹•æ™‚ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— + æ®µéšçš„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ + ãƒªãƒˆãƒ©ã‚¤
"""

import asyncio
import json
import psutil
from datetime import datetime
import os
import sys
import time
import requests

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
import uvicorn

# ===== Ollama endpoints/config =====
# åŸºæœ¬URLï¼ˆPORT/ãƒ›ã‚¹ãƒˆã‚’ç’°å¢ƒå¤‰æ•°ã§å·®ã—æ›¿ãˆå¯èƒ½ï¼‰
OLLAMA_BASE = os.environ.get("OLLAMA_BASE", "http://127.0.0.1:11434")
OLLAMA_GENERATE_URL = os.environ.get("OLLAMA_URL", f"{OLLAMA_BASE}/api/generate")
OLLAMA_CHAT_URL = os.environ.get("OLLAMA_CHAT_URL", f"{OLLAMA_BASE}/api/chat")
OLLAMA_TAGS_URL = f"{OLLAMA_BASE}/api/tags"

OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "llama3.1:8b-instruct-q4_K_M")
OLLAMA_NUM_PREDICT = int(os.environ.get("OLLAMA_NUM_PREDICT", "80"))
OLLAMA_TEMPERATURE = float(os.environ.get("OLLAMA_TEMPERATURE", "0.2"))
OLLAMA_TOP_P = float(os.environ.get("OLLAMA_TOP_P", "0.9"))
OLLAMA_REPEAT_PENALTY = float(os.environ.get("OLLAMA_REPEAT_PENALTY", "1.1"))
OLLAMA_STOP = os.environ.get("OLLAMA_STOP", "\\n\\n").encode("utf-8").decode("unicode_escape")
# åˆå›ãƒ­ãƒ¼ãƒ‰ãŒé‡ã„ã“ã¨ãŒã‚ã‚‹ãŸã‚åºƒã‚ã®æ—¢å®šï¼ˆæ®µéšçš„ã«å»¶ã°ã™ï¼‰
OLLAMA_TIMEOUT_SEC = int(os.environ.get("OLLAMA_TIMEOUT_SEC", "180"))
OLLAMA_MAX_TIMEOUT_SEC = int(os.environ.get("OLLAMA_MAX_TIMEOUT_SEC", "360"))
OLLAMA_RETRIES = int(os.environ.get("OLLAMA_RETRIES", "2"))  # ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆå¤±æ•—å¾Œã«ã‚‚ã†ä¸€åº¦ï¼‰

# ===== FastAPI =====
app = FastAPI(title="Mac Status PWA - Ollama")

app.mount("/static", StaticFiles(directory="frontend"), name="static")

connected_clients = set()

# ====== System info ======
def get_system_info():
    try:
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')

        processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
            try:
                info = proc.info
                if info['cpu_percent'] is not None and info['cpu_percent'] > 0:
                    processes.append(info)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        processes = sorted(processes, key=lambda x: x['cpu_percent'] or 0, reverse=True)[:5]

        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": round(cpu_percent, 1),
            "memory_percent": round(memory.percent, 1),
            "memory_used": memory.used,
            "memory_total": memory.total,
            "disk_percent": round((disk.used / disk.total) * 100, 1),
            "disk_used": disk.used,
            "disk_total": disk.total,
            "processes": processes
        }
    except Exception as e:
        print(f"Error getting system info: {e}")
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": 0,
            "memory_percent": 0,
            "memory_used": 0,
            "memory_total": 1,
            "disk_percent": 0,
            "disk_used": 0,
            "disk_total": 1,
            "processes": [],
            "error": str(e)
        }

# ===== Broadcast loop =====
async def broadcast_system_status():
    while True:
        if connected_clients:
            system_info = get_system_info()
            message = {
                "type": "system_status_update",
                "data": system_info,
                "timestamp": datetime.now().isoformat()
            }
            disconnected = set()
            for ws in connected_clients:
                try:
                    await ws.send_text(json.dumps(message))
                except:
                    disconnected.add(ws)
            connected_clients.difference_update(disconnected)
        await asyncio.sleep(2)

# ===== Warmup =====
def _ollama_ping():
    try:
        requests.get(OLLAMA_TAGS_URL, timeout=10)
    except Exception as e:
        print(f"[warmup] tags ping error: {e}")

def _ollama_one_token_warmup():
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": "OK",
        "options": {"num_predict": 1},
        "keep_alive": "30m",
        "stream": False
    }
    try:
        requests.post(OLLAMA_GENERATE_URL, json=payload, timeout=60)
        print("[warmup] one-token generate done")
    except Exception as e:
        print(f"[warmup] one-token generate error: {e}")

async def warmup_ollama():
    print("ğŸ§Š Warmup Ollama ...")
    await asyncio.to_thread(_ollama_ping)
    await asyncio.to_thread(_ollama_one_token_warmup)
    print("ğŸ”¥ Warmup finished")

# ===== Startup =====
@app.on_event("startup")
async def startup_event():
    asyncio.create_task(broadcast_system_status())
    asyncio.create_task(warmup_ollama())
    print(f"ğŸ§  OLLAMA_BASE : {OLLAMA_BASE}")
    print(f"ğŸ§© MODEL       : {OLLAMA_MODEL}")

# ===== LLM helpers =====
def _ollama_post(url: str, payload: dict) -> str:
    """æ®µéšçš„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ + ãƒªãƒˆãƒ©ã‚¤ã€‚æˆåŠŸãªã‚‰ãƒ†ã‚­ã‚¹ãƒˆã€å¤±æ•—ã¯ç©ºæ–‡å­—ã€‚"""
    timeouts = [OLLAMA_TIMEOUT_SEC]
    if OLLAMA_RETRIES > 0:
        timeouts.append(min(OLLAMA_MAX_TIMEOUT_SEC, OLLAMA_TIMEOUT_SEC * 2))

    for i, t in enumerate(timeouts):
        try:
            r = requests.post(url, json=payload, timeout=t)
            r.raise_for_status()
            data = r.json()
            # /api/generate ã¯ 'response'ã€/api/chat ã¯ 'message' å†… 'content'
            if "response" in data:
                return (data.get("response") or "").strip()
            if "message" in data and isinstance(data["message"], dict):
                return (data["message"].get("content") or "").strip()
            # chat ã® stream=False ã§ã‚‚ messages é…åˆ—ãŒè¿”ã‚‹å ´åˆãŒã‚ã‚‹
            if "messages" in data and data["messages"]:
                last = data["messages"][-1]
                if isinstance(last, dict):
                    return (last.get("content") or "").strip()
            return ""
        except requests.exceptions.ReadTimeout as e:
            print(f"[Ollama timeout {t}s try#{i+1}] {e}")
            if i == 0:
                # 1ç™ºç›®ã§è©°ã¾ã£ãŸã‚‰è»½ãã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’å†å®Ÿè¡Œ
                try:
                    _ollama_one_token_warmup()
                except Exception:
                    pass
            continue
        except Exception as e:
            print(f"[Ollama error try#{i+1}] {e}")
            time.sleep(0.2)
            continue
    return ""

def build_system_prompt(user_query: str, system_metrics: dict) -> str:
    cpu = system_metrics.get("cpu_percent", 0)
    mem = system_metrics.get("memory_percent", 0)
    mem_used_gb = system_metrics.get("memory_used", 0) / (1024**3)
    mem_total_gb = system_metrics.get("memory_total", 1) / (1024**3)
    disk = system_metrics.get("disk_percent", 0)
    disk_used_gb = system_metrics.get("disk_used", 0) / (1024**3)
    disk_total_gb = system_metrics.get("disk_total", 1) / (1024**3)
    top = system_metrics.get("processes", [])[:3]

    top_lines = []
    for i, p in enumerate(top, 1):
        top_lines.append(f"{i}. {p.get('name','?')} | CPU {p.get('cpu_percent',0):.1f}% / MEM {p.get('memory_percent',0):.1f}%")
    top_block = "\n".join(top_lines) if top_lines else "ï¼ˆä¸Šä½ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ãªã—ï¼‰"

    prompt = (
        "ã‚ãªãŸã¯ macOS ã®è»½é‡ã‚·ã‚¹ãƒ†ãƒ ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å°‚é–€ç”¨èªã‚’é¿ã‘ã€æ—¥æœ¬èªã§ç°¡æ½”ãƒ»è¦ç‚¹ã®ã¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚\n"
        "å–å¾—ã§ããªã„æƒ…å ±ã¯ã€æœªå¯¾å¿œã€ã¨è¿°ã¹ã€ä»£æ›¿ã®ç¢ºèªæ‰‹æ®µã‚’1ã€œ2å€‹ã ã‘ææ¡ˆã—ã¦ãã ã•ã„ã€‚çµµæ–‡å­—ã¯æœ€å°é™ã€‚\n\n"
        "=== ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ ===\n"
        f"- CPUä½¿ç”¨ç‡: {cpu:.1f}%\n"
        f"- ãƒ¡ãƒ¢ãƒª: {mem:.1f}%ï¼ˆ{mem_used_gb:.1f}GB / {mem_total_gb:.1f}GBï¼‰\n"
        f"- ãƒ‡ã‚£ã‚¹ã‚¯: {disk:.1f}%ï¼ˆ{disk_used_gb:.0f}GB / {disk_total_gb:.0f}GBï¼‰\n"
        f"- ä¸Šä½ãƒ—ãƒ­ã‚»ã‚¹:\n{top_block}\n"
        "=========================\n\n"
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_query}\n"
        "å›ç­”ã¯2ã€œ6è¡Œç¨‹åº¦ã§ã€‚"
    )
    return prompt

def generate_with_ollama_generate(prompt: str) -> str:
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "options": {
            "num_predict": OLLAMA_NUM_PREDICT,
            "temperature": OLLAMA_TEMPERATURE,
            "top_p": OLLAMA_TOP_P,
            "repeat_penalty": OLLAMA_REPEAT_PENALTY,
            "stop": [OLLAMA_STOP] if OLLAMA_STOP else []
        },
        "keep_alive": "30m",
        "stream": False
    }
    return _ollama_post(OLLAMA_GENERATE_URL, payload)

def build_smalltalk_messages(user_text: str):
    system = (
        "ã‚ãªãŸã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªä¼šè©±ç›¸æ‰‹ã§ã™ã€‚æ—¥æœ¬èªã§ä¸å¯§ã ãŒå …ã™ããªã„èª¿å­ã§ã€1ã€œ4æ–‡ã§è¿”ç­”ã€‚"
        "çŠ¶æ³ã«å¿œã˜ã¦çŸ­ã„ç›¸ã¥ã¡ã‚„1ã¤ã ã‘ç°¡å˜ãªè³ªå•ã§ãƒ©ãƒªãƒ¼ã‚’ç¶šã‘ã¦ã‚‚OKã€‚çµµæ–‡å­—ã¯æœ€å°é™ã€‚"
    )
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user_text}
    ]

def generate_with_ollama_chat(messages) -> str:
    payload = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "options": {
            "num_predict": max(48, OLLAMA_NUM_PREDICT),  # é›‘è«‡ã¯å°‘ã—é•·ã‚ã§ã‚‚OK
            "temperature": 0.5,  # é›‘è«‡ã¯å°‘ã—è‡ªç„¶ã•å„ªå…ˆ
            "top_p": 0.9,
            "repeat_penalty": 1.05
        },
        "keep_alive": "30m",
        "stream": False
    }
    return _ollama_post(OLLAMA_CHAT_URL, payload)

def is_smalltalk(text: str) -> bool:
    t = (text or "").strip()
    if not t:
        return False
    greetings = ["ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã°ã‚“ã¯", "ãŠã¯ã‚ˆã†", "ã‚„ã‚", "ã¯ã‚ãƒ¼", "hi", "hello"]
    pleasantries = ["ã‚ã‚ŠãŒã¨ã†", "åŠ©ã‹ã£ãŸ", "åŠ©ã‹ã‚Šã¾ã™", "ãŠã¤ã‹ã‚Œ", "ãŠç–²ã‚Œ", "å…ƒæ°—", "ã©ã†ã‚‚", "ãªã‚‹ã»ã©", "äº†è§£", "ã‚Šã‚‡ã†ã‹ã„"]
    sys_keywords = ["cpu", "ãƒ¡ãƒ¢ãƒª", "memory", "ram", "ãƒ‡ã‚£ã‚¹ã‚¯", "disk", "storage",
                    "wifi", "wi-fi", "ãƒãƒƒãƒˆ", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ", "ãƒ—ãƒ­ã‚»ã‚¹", "æ¸©åº¦", "fan", "ãƒãƒƒãƒ†ãƒªãƒ¼", "battery"]

    if any(k in t for k in greetings) or any(k in t for k in pleasantries):
        return True
    if any(k in t.lower() for k in sys_keywords):
        return False
    # çŸ­ã„ç™ºè©±ã¯é›‘è«‡ã¨ã¿ãªã™
    return len(t) <= 40

# ===== Fallback (å¾“æ¥ã©ãŠã‚Š) =====
def generate_fallback_response(user_message: str, system_info: dict) -> str:
    user_message_lower = (user_message or "").lower()
    cpu_usage = system_info.get("cpu_percent", 0)
    memory_percent = system_info.get("memory_percent", 0)
    memory_used_gb = system_info.get("memory_used", 0) / (1024**3)
    memory_total_gb = system_info.get("memory_total", 1) / (1024**3)
    disk_percent = system_info.get("disk_percent", 0)
    disk_used_gb = system_info.get("disk_used", 0) / (1024**3)
    disk_total_gb = system_info.get("disk_total", 1) / (1024**3)
    top_processes = system_info.get("processes", [])[:3]

    def contains(text, kws): return any(k in text for k in kws)

    if contains(user_message_lower, ["ãƒãƒƒãƒ†ãƒªãƒ¼", "battery", "é›»æ± ", "å……é›»"]):
        import random
        return random.choice([
            "ğŸ”‹ ç¾åœ¨ãƒãƒƒãƒ†ãƒªãƒ¼æƒ…å ±ã®å–å¾—ã¯æœªå¯¾å¿œã§ã™ã€‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒãƒ¼ã®ãƒãƒƒãƒ†ãƒªãƒ¼ã‚¢ã‚¤ã‚³ãƒ³ã‚„è¨­å®š>ãƒãƒƒãƒ†ãƒªãƒ¼ã‚’ã”ç¢ºèªãã ã•ã„ã€‚",
            "ğŸ”‹ ãƒãƒƒãƒ†ãƒªãƒ¼ç›£è¦–ã¯æœªå®Ÿè£…ã§ã™ã€‚è¨­å®š>ãƒãƒƒãƒ†ãƒªãƒ¼ã€ã‚‚ã—ãã¯ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ¢ãƒ‹ã‚¿>ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
        ])

    elif contains(user_message_lower, ["ã‚¢ãƒ—ãƒª", "app", "ãƒ—ãƒ­ã‚»ã‚¹", "process", "å®Ÿè¡Œä¸­", "èµ·å‹•ä¸­", "å‹•ã„ã¦ã„ã‚‹"]):
        s = "ğŸ“± **ç¾åœ¨å®Ÿè¡Œä¸­ã®ä¸»è¦ãƒ—ãƒ­ã‚»ã‚¹:**\n\n"
        if top_processes:
            for i, p in enumerate(top_processes, 1):
                s += f"**{i}. {p.get('name')}**  CPU {p.get('cpu_percent',0):.1f}% / MEM {p.get('memory_percent',0):.1f}%\n"
        else:
            s += "å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
        s += f"\nğŸ“Š å…¨ä½“: CPU {cpu_usage}% / ãƒ¡ãƒ¢ãƒª {memory_percent}%"
        return s

    elif contains(user_message_lower, ["wifi", "wi-fi", "ãƒ¯ã‚¤ãƒ•ã‚¡ã‚¤", "ç„¡ç·š", "ãƒãƒƒãƒˆ", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ", "æ¥ç¶š"]):
        return f"ğŸ“¶ Wi-Fiè©³ç´°å–å¾—ã¯æœªå¯¾å¿œã§ã™ã€‚è¨­å®š>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒãƒ¼ã®Wi-Fiã‹ã‚‰ç¢ºèªã—ã¦ãã ã•ã„ã€‚\nç¾åœ¨: CPU {cpu_usage}% / MEM {memory_percent}%"

    elif contains(user_message_lower, ["cpu", "ãƒ—ãƒ­ã‚»ãƒƒã‚µ", "ä½¿ç”¨ç‡", "å‡¦ç†"]):
        s = f"ğŸ–¥ï¸ ç¾åœ¨ã®CPUä½¿ç”¨ç‡ã¯ {cpu_usage}% ã§ã™ã€‚\n"
        if cpu_usage > 80: s += "âš ï¸ é«˜è² è·ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        elif cpu_usage > 50: s += "ğŸ“Š ä¸­ç¨‹åº¦ã®è² è·ã§ã™ã€‚"
        else: s += "âœ… ä½è² è·ã§ã™ã€‚"
        return s

    elif contains(user_message_lower, ["ãƒ¡ãƒ¢ãƒª", "memory", "ram", "ä½¿ç”¨é‡"]):
        s = (f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory_percent}%\n"
             f"ä½¿ç”¨é‡: {memory_used_gb:.1f}GB / {memory_total_gb:.1f}GB")
        return s

    elif contains(user_message_lower, ["ãƒ‡ã‚£ã‚¹ã‚¯", "disk", "storage", "å®¹é‡"]):
        s = (f"ğŸ’¿ ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡: {disk_percent}%\n"
             f"ä½¿ç”¨é‡: {disk_used_gb:.0f}GB / {disk_total_gb:.0f}GB")
        return s

    elif contains(user_message_lower, ["ã‚·ã‚¹ãƒ†ãƒ ", "status", "å…¨ä½“", "çŠ¶æ³"]):
        return (f"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“\n"
                f"CPU {cpu_usage}%, ãƒ¡ãƒ¢ãƒª {memory_percent}%, ãƒ‡ã‚£ã‚¹ã‚¯ {disk_percent}%")

    elif contains(user_message_lower, ["ã“ã‚“ã«ã¡ã¯", "hello", "ã¯ã˜ã‚ã¾ã—ã¦", "ãŠã¯ã‚ˆã†", "ã“ã‚“ã°ã‚“ã¯"]):
        return (f"ğŸ‘‹ ã“ã‚“ã«ã¡ã¯ï¼ç¾åœ¨ã®çŠ¶æ³: CPU {cpu_usage}%, ãƒ¡ãƒ¢ãƒª {memory_percent}%, ãƒ‡ã‚£ã‚¹ã‚¯ {disk_percent}%\n"
                "æ°—ã«ãªã‚‹é …ç›®ãŒã‚ã‚Œã°ã©ã†ãã€‚")

    else:
        return (f"ğŸ” çŠ¶æ…‹: CPU {cpu_usage}% / ãƒ¡ãƒ¢ãƒª {memory_percent}% / ãƒ‡ã‚£ã‚¹ã‚¯ {disk_percent}%\n"
                "ã€CPUã®è©³ç´°ã€ã€å®Ÿè¡Œä¸­ã®ã‚¢ãƒ—ãƒªã€ãªã©å…·ä½“çš„ã«ã©ã†ãã€‚")

# ===== Routes =====
@app.get("/")
async def serve_pwa():
    try:
        with open("frontend/index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read(), status_code=200)
    except FileNotFoundError:
        return HTMLResponse("<h1>Mac Status PWA</h1><p>Frontend files not found</p>", status_code=200)

@app.get("/fixed")
async def serve_fixed_pwa():
    try:
        with open("fixed_index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read(), status_code=200)
    except FileNotFoundError:
        return HTMLResponse("<h1>Fixed Mac Status PWA</h1><p>Fixed version not found</p>", status_code=200)

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

# ===== WebSocket =====
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    connected_clients.add(websocket)
    print(f"WebSocket client connected. Total: {len(connected_clients)}")

    # åˆå›ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    try:
        system_info = get_system_info()
        await websocket.send_text(json.dumps({
            "type": "system_status_response",
            "data": {"system_status": system_info},
            "timestamp": datetime.now().isoformat()
        }))
    except Exception as e:
        print(f"Error sending initial status: {e}")

    try:
        while True:
            data = await websocket.receive_text()
            print(f"Received: {data}")
            try:
                msg = json.loads(data)
                t = msg.get("type", "")

                if t == "ping":
                    await websocket.send_text(json.dumps({"type": "pong", "timestamp": datetime.now().isoformat()}))

                elif t == "system_status_request":
                    system_info = get_system_info()
                    await websocket.send_text(json.dumps({
                        "type": "system_status_response",
                        "data": {"system_status": system_info},
                        "timestamp": datetime.now().isoformat()
                    }))

                elif t == "chat_message":
                    # äº’æ›: data.message or message
                    user_message = ""
                    if "data" in msg and isinstance(msg["data"], dict):
                        user_message = msg["data"].get("message", "")
                    else:
                        user_message = msg.get("message", "")

                    print(f"ğŸ” chat: '{user_message}'")
                    system_info = get_system_info()

                    # é›‘è«‡ã‹ã©ã†ã‹ã§åˆ†å²
                    try_text = ""
                    if is_smalltalk(user_message):
                        # è»½é‡ãªé›‘è«‡ã¯ /api/chat
                        messages = build_smalltalk_messages(user_message)
                        try_text = generate_with_ollama_chat(messages)
                        if not try_text:
                            # å¿µã®ãŸã‚ generate ã§ã‚‚è©¦ã™
                            prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_message}\nè‡ªç„¶ãªæ—¥æœ¬èªã§1ã€œ4æ–‡ã§è¿”ç­”ã€‚"
                            try_text = generate_with_ollama_generate(prompt)
                    else:
                        # ã‚·ã‚¹ãƒ†ãƒ è³ªå•ã¯ generate + ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                        prompt = build_system_prompt(user_message, system_info)
                        try_text = generate_with_ollama_generate(prompt)

                    if not try_text:
                        try_text = generate_fallback_response(user_message, system_info)
                        print(f"âœ… fallback used ({len(try_text)} chars)")
                    else:
                        print(f"âœ… llm used ({len(try_text)} chars)")

                    await websocket.send_text(json.dumps({
                        "type": "chat_response",
                        "data": {"message": try_text},
                        "timestamp": datetime.now().isoformat()
                    }))

                else:
                    await websocket.send_text(json.dumps({
                        "type": "error",
                        "data": {"message": f"Unknown message type: {t}"},
                        "timestamp": datetime.now().isoformat()
                    }))

            except json.JSONDecodeError:
                await websocket.send_text(json.dumps({
                    "type": "error",
                    "data": {"message": "Invalid JSON format"},
                    "timestamp": datetime.now().isoformat()
                }))

    except WebSocketDisconnect:
        connected_clients.discard(websocket)
        print(f"WebSocket client disconnected. Total: {len(connected_clients)}")

# ===== Main =====
if __name__ == "__main__":
    print("ğŸš€ Starting Mac Status PWA Server (Ollama+Chat)")
    print(f"BASE   : {OLLAMA_BASE}")
    print(f"MODEL  : {OLLAMA_MODEL}")
    print(f"TIMEOUT: {OLLAMA_TIMEOUT_SEC}s (max {OLLAMA_MAX_TIMEOUT_SEC}s) RETRIES: {OLLAMA_RETRIES}")
    uvicorn.run(app, host="0.0.0.0", port=8002)
